---
title: "Practical Machine Learning Project"
author: "Brian Crilly"
date: "10/27/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
This project explores the use of various machine learning algorithms on the Human Activity Recognition data set. The data set contains accelerometer data for participants, along with a determination of how well the participant performed each exercise (Class A through E).

The data sets for this project are found here:  
*[Training Data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)  
*[Test Data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)  

The training data is used to train 5 separate models: **Decision Trees, Random Forest, Gradient Boosted Trees, Linear Discriminant Analysis, and Naive Bayes.** The results of these models are compared for prediction accuracy. The most accurate models are found to be **Random Forest** and **Gradient Boosted Trees**. These two models are used to make predictions using the test data.

## Setup
The first step in the analysis process is to load the data and libraries.

```{r data_init, message = FALSE}
library(caret)
library(data.table)
library(doParallel)
library(RANN)
library(ranger)
library(rattle)
```
```{r}
trainFileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testFileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

if(!file.exists("data")) {
    dir.create("data")
}

download.file(trainFileUrl, destfile = "./data/trainFile.csv", method = "curl")
download.file(testFileUrl, destfile = "./data/testFile.csv", method = "curl")

trainData <- read.table("./data/trainFile.csv", sep = ",", header = TRUE)
testData <- read.table("./data/testFile.csv", sep = ",", header = TRUE)

dim(trainData)
dim(testData)
```

## Cleaning the Data
The training data set needs to be cleaned and preprocessed as follows:  
1. Remove the first 7 columns of non-relevant data.  
2. Remove columns for which the majority of the data is NA.  
3. Remove columns that have limited variance and, therefore, limited predictive use.  
4. Split the training data into a training set to train the models and a validation set to validate model accuracy.    

```{r data_clean}
# Remove the first 7 columns of non-relevant data
trainData <- trainData[, -c(1:7)]

# Remove columns for which more than half of the data is NA
naRMIndex <- colMeans(is.na(trainData)) > 0.5
trainData <- trainData[ , !naRMIndex]

# Remove columns for which there is little to no variance in the data
nzvIndex <- nearZeroVar(trainData)
trainData <- trainData[ , -nzvIndex]

set.seed(54321)

# Split the training set into training data and validation data
trainSel <- createDataPartition(y = trainData$classe, p = 0.7, list = FALSE)
validationData <- trainData[-trainSel, ]
trainData <- trainData[trainSel, ]
```

## Training the Models
In this section, each model will be trained using the training data. Note that parallel processing is invoked to help enhance the speed of model creation.  Further, 3-fold cross validation is selected.

### Training Setup

```{r train_setup}
# Set up for parallel processing. Leave one core free to reduce likelihood of crashing
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

# Set up for cross validation and allow parallel processing
control <- trainControl(method = "cv", number = 3, verboseIter = FALSE, allowParallel = TRUE)
```

### Decision Tree Training

```{r dt_train}
# Train Decision Tree - Set tuneLength to 5 to try to predict all possible outcomes (A through E)
dtModelFit <- train(classe ~ ., data = trainData, method = "rpart", tuneLength = 5,
                    trControl = control)
```

Here is the graphical view of the decision tree model.

```{r dt_model}
fancyRpartPlot(dtModelFit$finalModel)
```

### Random Forest Training

```{r rf_train}
# Train Random Forest
rfModelFit <- train(classe ~ ., data = trainData, method = "rf", prox = TRUE, trControl = control)
```

### Gradient Boosted Tree Training

```{r gbm_train}
# Train Gradient Boosted Tree
gbmModelFit <- train(classe ~ ., data = trainData, method = "gbm", verbose = FALSE,
                     trControl = control)
```

### Lineaer Discriminant Analysis Training

```{r lda_train}
# Train Linear Discriminant Analysis
ldaModelFit <- train(classe ~ ., data = trainData, method = "lda", trControl = control)
```

### Naive Bayes Training

```{r nb_train}
# Train Naive Bayes
nbModelFit <- train(classe ~ ., data = trainData, method = "nb", trControl = control)
```

### Training Cleanup

```{r}
# Stop Parallel Processing
stopCluster(cluster)
```

## Validating the Models
For each model, predictions are made using the validation data, and confusion matrices are presented to assess the performance of each model on the validation data sets.  

### Decision Tree Results

```{r dt_CM}
dtPredict <- predict(dtModelFit, validationData)
dtCM <- confusionMatrix(dtPredict, factor(validationData$classe))
dtCM
```

### Random Forest Results

```{r rf_CM}
rfPredict <- predict(rfModelFit, validationData)
rfCM <- confusionMatrix(rfPredict, factor(validationData$classe))
rfCM
```

### Gradient Boosted Trees Results

```{r gbm_CM}
gbmPredict <- predict(gbmModelFit, validationData)
gbmCM <- confusionMatrix(gbmPredict, factor(validationData$classe))
gbmCM
```

### Linear Discriminant Analysis Results

```{r lda_CM}
ldaPredict <- predict(ldaModelFit, validationData)
ldaCM <- confusionMatrix(ldaPredict, factor(validationData$classe))
ldaCM
```

### Naive Bayes Results

```{r nb_CM, warning = FALSE}
nbPredict <- predict(nbModelFit, validationData)
nbCM <- confusionMatrix(nbPredict, factor(validationData$classe))
nbCM
```

## Results Summary

Algorithm | Accuracy | Out of Sample Error
----------|---------:|--------------------:
Decision Tree | `r dtCM$overall["Accuracy"]` | `r 1 - dtCM$overall["Accuracy"]`
Random Forest | `r rfCM$overall["Accuracy"]` | `r 1 - rfCM$overall["Accuracy"]`
Gradient Boosted Trees | `r gbmCM$overall["Accuracy"]` | `r 1 - gbmCM$overall["Accuracy"]`
Linear Discriminant Analysis | `r ldaCM$overall["Accuracy"]` | `r 1 - ldaCM$overall["Accuracy"]`
Naive Bayes | `r nbCM$overall["Accuracy"]` | `r 1 - nbCM$overall["Accuracy"]`

From the results listed above, it is apparent that **Random Forest** and **Gradient Boosted Trees** perform the best out of the models evaluated, with the highest accuracy and lowest out of sample error.

## Prediction on Test Set

Predictions are run against the test set using both the Random Forest and the Gradient Boosted Trees models.  

### Random Forest Prediction

```{r rf_predict}
rfTestPredict <- predict(rfModelFit, testData)
rfTestPredict

```

### Gradient Boosted Trees Prediction

```{r gbm_predict}
gbmTestPredict <- predict(gbmModelFit, testData)
gbmTestPredict
```

### Prediction Comparison

The predictions for both models match 
`r sprintf("%1.1f%%", mean(rfTestPredict == gbmTestPredict) * 100)`.